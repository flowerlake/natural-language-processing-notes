{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyltp 文档链接：https://pyltp.readthedocs.io/zh_CN/latest/api.html#id17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取孟晚舟事件的标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filename = np.array([])\n",
    "for root, dirs, files in os.walk('WeChatArticles/'):\n",
    "    filename = np.concatenate((filename,np.array([name.split(\".\")[0] for name in files])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'315曝光名单来了，电子烟危害不比香烟小，孟晚舟在华为不开心'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315曝光名单来了，电子烟危害不比香烟小，孟晚舟在华为不开心\n",
      "315\t曝光名单\t来\t了\t，\t电子烟\t危害\t不\t比\t香烟\t小\t，\t孟晚舟\t在\t华为\t不\t开心\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "\n",
    "import os\n",
    "\n",
    "# sentence = '元芳你怎么看？我就趴窗口上看呗！'\n",
    "sentence = filename[0]\n",
    "\n",
    "sents = SentenceSplitter.split(sentence)  # 分句\n",
    "print('\\t'.join(sents))\n",
    "\n",
    "\n",
    "LTP_DATA_DIR = 'pyltp_model/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "\n",
    "\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "\n",
    "# SentenceSplitter分句子，把段落拆分成句子；segmentor.segment分词\n",
    "# 如果要使用外部词典，指定新的字典集：\n",
    "segmentor.load_with_lexicon(cws_model_path, 'userdict.txt') # 加载模型，第二个参数是您的外部词典文件路径\n",
    "\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "words = segmentor.segment(sentence)  # 分词\n",
    "print('\\t'.join(words))\n",
    "segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyltp 词性标注同样支持用户的外部词典。词性标注外部词典同样为一个文本文件，每行指定一个词，第一列指定单词，第二列之后指定该词的候选词性（可以有多项，每一项占一列），列与列之间用空格区分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m\tn\tv\tu\twp\tn\tv\td\tp\tn\ta\twp\tnh\tp\tnz\td\ta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "postags = postagger.postag(words)  # 词性标注\n",
    "\n",
    "print('\\t'.join(postags))\n",
    "postagger.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tS-Nh\tO\tO\tO\tO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyltp import NamedEntityRecognizer\n",
    "\n",
    "LTP_DATA_DIR = 'pyltp_model/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "\n",
    "print('\\t'.join(netags))\n",
    "recognizer.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:ATT\t3:SBV\t0:HED\t3:RAD\t3:WP\t7:SBV\t3:COO\t11:ADV\t11:ADV\t9:POB\t7:VOB\t7:WP\t17:SBV\t17:ADV\t14:POB\t17:ADV\t7:COO\n"
     ]
    }
   ],
   "source": [
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "arcs = parser.parse(words, postags)  # 句法分析\n",
    "\n",
    "print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "parser.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyltp.VectorOfString at 0x1eb82a09cf0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语义角色分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心的语义角色为 A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。其余的15个语义角色为附加语义角色，如LOC 表示地点，TMP 表示时间等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 A0:(0,1)\n",
      "10 ADV:(7,7)ADV:(8,9)\n",
      "16 A0:(12,12)LOC:(13,14)ADV:(15,15)\n"
     ]
    }
   ],
   "source": [
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n",
    "\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型\n",
    "\n",
    "\n",
    "# arcs 使用依存句法分析的结果\n",
    "roles = labeller.label(words, postags,arcs)  # 语义角色标注\n",
    "\n",
    "# 打印结果\n",
    "for role in roles:\n",
    "    print(role.index, \"\".join([\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "labeller.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315,0\t曝光名单,1\t来,2\t了,3\t，,4\t电子烟,5\t危害,6\t不,7\t比,8\t香烟,9\t小,10\t，,11\t孟晚舟,12\t在,13\t华为,14\t不,15\t开心,16\t"
     ]
    }
   ],
   "source": [
    "for item,i in zip(words,range(len(words))):\n",
    "    print(\"{item},{num}\".format(item=item,num=i),end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
